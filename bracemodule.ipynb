{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNJOj9RASpT1165+tLKdwEl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vidzshan/vidzshan/blob/main/bracemodule.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting Google Drive"
      ],
      "metadata": {
        "id": "9K0Ht4f2Rl7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "MOUNTPOINT = '/content/gdrive'\n",
        "DATADIR = os.path.join(MOUNTPOINT, 'My Drive', 'Colab Notebooks','Research')\n",
        "drive.mount(MOUNTPOINT)\n",
        "\n",
        "#Access the data\n",
        "#path = os.path.join(DATADIR, 'data')\n",
        "\n",
        "#save trained data results or modules\n",
        "#model.save(os.path.join(DATADIR, 'model.h5'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fwNGSF6RlEP",
        "outputId": "050aaf8e-2535-4100-a3f8-c18f905618cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuDGaTSfNiL1",
        "outputId": "e3bde562-8187-4899-f1b1-3396bf771d4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMPL-X loaded successfully!\n",
            "Output vertices shape: torch.Size([1, 10475, 3])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import smplx\n",
        "\n",
        "model_path = os.path.join(DATADIR, 'smplx_models','SMPLX_NEUTRAL_2020.npz')\n",
        "\n",
        "# Load BRACE keypoint sample\n",
        "keypoints_path = os.path.join(DATADIR, 'brace','manual_keypoints', '2011', '3rIk56dcBTM', 'img-001293.npz')\n",
        "keypoints_data = np.load(keypoints_path)\n",
        "keypoints = keypoints_data['coco_joints2d'][:, :2]  # Shape: [17, 2] for x, y coords\n",
        "\n",
        "# Install SMPL-X dependencies\n",
        "#!pip install smplx pyrender trimesh\n",
        "#!git clone https://github.com/vchoutas/smplx /content/smplx\n",
        "smpl_model = smplx.create(model_path=model_path, model_type='smplx', gender='neutral', use_pca=False, num_betas = 10, num_expression_coeffs=10).to('cuda')\n",
        "print(\"SMPL-X loaded successfully!\")\n",
        "# Forward pass with correct tensor shapes\n",
        "output = smpl_model(\n",
        "    betas=torch.zeros(1, 10).to('cuda'),\n",
        "    expression=torch.zeros(1, 10).to('cuda'),\n",
        "    body_pose=torch.zeros(1, 21*3).to('cuda'),  # 21 joints, 3D rotation each (not 21*6)\n",
        "    global_orient=torch.zeros(1, 3).to('cuda')   # Add global_orient [1, 3]\n",
        ")\n",
        "print(f\"Output vertices shape: {output.vertices.shape}\")  # Should be [1, 10475, 3]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install torch-geometric"
      ],
      "metadata": {
        "id": "sI7Xa8DRVHST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Load segments.csv\n",
        "segments = pd.read_csv(Path(DATADIR) / 'brace' / 'annotations' / 'segments.csv')\n",
        "sequences = segments.head(30)  # 30 sequences\n",
        "\n",
        "# Define COCO skeleton edges (17 joints)\n",
        "edge_index = torch.tensor([\n",
        "    [0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7],  # Head to arms\n",
        "    [1,8], [8,9], [9,10], [10,11], [8,12], [12,13], [13,14],  # Torso to legs\n",
        "    [0,15], [0,16]  # Nose to eyes\n",
        "], dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Prepare data\n",
        "data_list = []\n",
        "for _, row in sequences.iterrows():\n",
        "    keypoints_file = Path(DATADIR) / 'brace' / 'dataset' / str(row['year']) / row['video_id'] / f\"{row['video_id']}_{row['start_frame']}-{row['end_frame']}_{row['dance_type']}.json.json\"\n",
        "    with open(keypoints_file, 'r') as f:\n",
        "        keypoints_dict = json.load(f)\n",
        "    keypoints_array = np.array([v['keypoints'] for v in keypoints_dict.values()])[:, :, :2]  # [frames, 17, 2]\n",
        "    box = list(keypoints_dict.values())[0]['box']  # [x, y, w, h, score]\n",
        "    keypoints_norm = (keypoints_array - [box[0], box[1]]) / [box[2], box[3]]  # Normalize\n",
        "\n",
        "    # Compute velocity\n",
        "    velocity = np.diff(keypoints_norm, axis=0, prepend=keypoints_norm[:1])  # [frames, 17, 2]\n",
        "\n",
        "    # Load audio features\n",
        "    audio_file = Path(DATADIR) / 'brace' / 'audio_features' / str(row['year']) / row['video_id'] / f\"{row['video_id']}.{row['seq_idx']}.npz\"\n",
        "    if audio_file.exists():\n",
        "        audio_data = np.load(audio_file)\n",
        "        onset_beat = audio_data['onset_beat'].flatten()  # Ensure 1D array\n",
        "        if len(onset_beat) > 1:  # Ensure enough points for interpolation\n",
        "            onset_beat = np.interp(\n",
        "                np.arange(len(keypoints_norm)),\n",
        "                np.linspace(0, len(onset_beat)-1, len(onset_beat)),\n",
        "                onset_beat\n",
        "            )  # Align to keypoint frames\n",
        "            onset_beat = onset_beat[:, np.newaxis, np.newaxis]  # [frames, 1, 1]\n",
        "            x = np.concatenate([keypoints_norm.reshape(-1, 17*2), velocity.reshape(-1, 17*2), onset_beat.reshape(-1, 1)], axis=1)  # [frames, 69]\n",
        "        else:\n",
        "            x = np.concatenate([keypoints_norm.reshape(-1, 17*2), velocity.reshape(-1, 17*2)], axis=1)  # [frames, 68]\n",
        "    else:\n",
        "        x = np.concatenate([keypoints_norm.reshape(-1, 17*2), velocity.reshape(-1, 17*2)], axis=1)  # [frames, 68]\n",
        "\n",
        "    x = torch.tensor(x, dtype=torch.float)\n",
        "    y = torch.tensor(row['dance_type_id'], dtype=torch.long)\n",
        "    data = Data(x=x, edge_index=edge_index, y=y)\n",
        "    data_list.append(data)"
      ],
      "metadata": {
        "id": "-HJ01Mmc0CfK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GCN model\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(69, 64)  # 34 (keypoints) + 34 (velocity) + 1 (onset_beat)\n",
        "        self.conv2 = GCNConv(64, 3)   # 3 classes\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch  # Add batch for batched graphs\n",
        "        x = torch.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        # Pool per graph in batch using global_mean_pool\n",
        "        from torch_geometric.nn import global_mean_pool\n",
        "        x = global_mean_pool(x, batch)  # Shape: [batch_size, 3]\n",
        "        return x\n",
        "\n",
        "# Train\n",
        "model = GCN().to('cuda')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "loader = DataLoader(data_list[:24], batch_size=4, shuffle=True)  # 80% train\n",
        "val_loader = DataLoader(data_list[24:], batch_size=4)  # 20% val\n",
        "\n",
        "model.train()\n",
        "for epoch in range(5):\n",
        "    for data in loader:\n",
        "        data = data.to('cuda')\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)  # Shape: [batch_size, 3] (e.g., [4, 3])\n",
        "        loss = criterion(out, data.y)  # data.y is [batch_size] (e.g., [4])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in val_loader:\n",
        "        data = data.to('cuda')\n",
        "        out = model(data)\n",
        "        pred = out.argmax(dim=1)  # Predict per batch\n",
        "        correct += (pred == data.y).sum().item()\n",
        "        total += len(data.y)\n",
        "    print(f\"Epoch {epoch+1}, Val Accuracy: {correct/total:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3Ncn5exkbVv",
        "outputId": "1b278a20-8151-4cf4-96a1-c8bbafeb4232"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Val Accuracy: 0.50\n",
            "Epoch 2, Val Accuracy: 0.50\n",
            "Epoch 3, Val Accuracy: 0.50\n",
            "Epoch 4, Val Accuracy: 0.50\n",
            "Epoch 5, Val Accuracy: 0.67\n"
          ]
        }
      ]
    }
  ]
}