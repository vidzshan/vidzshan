<h1 align="center">Hi üëã, I'm Vid-Zee</h1>
<h5 align="center">A researcher and developer specializing in human action recognition using deep learning, especially graph convolutional networks (GCNs) and skeleton data. My work spans skeleton-based annotation, pose estimation, and optimizing video datasets with Python, PyTorch, and cutting-edge annotation tools.</h5>
<img align="right" width="400" src="https://c4.wallpaperflare.com/wallpaper/958/66/509/death-babies-animation-dark-humor-old-people-children-bed-life-wallpaper-preview.jpg"
<p align="left"> <img src="https://komarev.com/ghpvc/?username=vidushanlnbti&label=Profile%20views&color=0e75b6&style=flat" alt="vidushanlnbti" /> </p>

<h3 align="left">Connect with me:</h3>
<p align="left">
<a href="https://www.linkedin.com/in/vidushan-prabash-aa8a36271/" target="_self"><img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/linked-in-alt.svg" alt="vidushan prabash" height="30" width="40" /></a>
<a href="https://web.facebook.com/vidushan.prabash" target="_self"><img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/facebook.svg" alt="vidushan prabash" height="30" width="40" /></a>
<a href="https://www.instagram.com/__vidu__shan__/" target="_self"><img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/instagram.svg" alt="__vidu__shan__" height="30" width="40" /></a>
<a href="https://youtube.com/@vidushanprabash4978" target="_self"><img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/youtube.svg" alt="vidushan prabash" height="30" width="40" /></a>
<a href="https://stackoverflow.com/users/20851054/vidushan" target="_self"><img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/stack-overflow.svg" alt="12462319" height="30" width="40" /></a>
<a href="https://leetcode.com/VidushanLNBTI/" target="_self"><img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/leet-code.svg" alt="layanyashoda" height="30" width="40" /></a>
<a href="https://codesandbox.io/dashboard/recent?workspace=854637cf-0f64-4c6c-8bec-2aff7a0e4d0f" target="_self"><img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/codesandbox.svg" alt="layanyashoda" height="30" width="40" /></a>



  
</p>

<h3 align="left">Languages and Tools:</h3>
<p align="left">
<h3 align="left">üöÄ Core Research & ML Tools</h3>
<p align="left">
  <a href="https://pytorch.org/" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/pytorch/pytorch-original.svg" alt="pytorch" width="40" height="40"/></a>
  <a href="https://jupyter.org/" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/jupyter/jupyter-original.svg" alt="jupyter" width="40" height="40"/></a>
  <a href="https://scikit-learn.org/" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/scikit-learn/scikit-learn-original.svg" alt="scikit-learn" width="40" height="40"/></a>
  <a href="https://colab.research.google.com/" target="_blank"><img src="https://upload.wikimedia.org/wikipedia/commons/3/32/Google_Colaboratory_SVG_Logo.svg" alt="colab" width="40" height="40"/></a>
  <a href="https://wandb.ai/" target="_blank"><img src="https://seeklogo.com/images/W/weighs-and-biases-logo-B4B040817E-seeklogo.com.png" alt="wandb" width="40" height="40"/></a>
</p>

<h3 align="left">üíª Programming Languages</h3>
<p align="left">
  <a href="https://www.python.org/" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/python/python-original.svg" alt="python" width="40" height="40"/></a>
  <a href="https://www.cprogramming.com/" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/c/c-original.svg" alt="c" width="40" height="40"/></a>
  <a href="https://www.w3schools.com/cpp/" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/cplusplus/cplusplus-original.svg" alt="c++" width="40" height="40"/></a>
  <a href="https://www.java.com" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/java/java-original.svg" alt="java" width="40" height="40"/></a>
  <a href="https://kotlinlang.org/" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/kotlin/kotlin-original.svg" alt="kotlin" width="40" height="40"/></a>
</p>

<h3 align="left">üß© Deep Learning & Data Science</h3>
<p align="left">
  <a href="https://pytorch.org/" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/pytorch/pytorch-original.svg" alt="pytorch" width="40" height="40"/></a>
  <a href="https://numpy.org/" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/numpy/numpy-original.svg" alt="numpy" width="40" height="40"/></a>
  <a href="https://scikit-learn.org/" target="_blank" rel="noreferrer">
    <img src="https://upload.wikimedia.org/wikipedia/commons/0/05/Scikit_learn_logo_small.svg" alt="scikit-learn" width="40" height="40"/></a>
  <a href="https://matplotlib.org/" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/matplotlib/matplotlib-original.svg" alt="matplotlib" width="40" height="40"/></a>
</p>

<h3 align="left">üõ†Ô∏è Frameworks/Frontend & Utilities</h3>
<p align="left">
  <a href="https://reactjs.org/" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/react/react-original-wordmark.svg" alt="react" width="40" height="40"/></a>
  <a href="https://nodejs.org" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/nodejs/nodejs-original-wordmark.svg" alt="nodejs" width="40" height="40"/></a>
  <a href="https://getbootstrap.com" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/bootstrap/bootstrap-plain-wordmark.svg" alt="bootstrap" width="40" height="40"/></a>
  <a href="https://tailwindcss.com/" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/tailwindcss/tailwindcss-plain.svg" alt="tailwind" width="40" height="40"/></a>
  <a href="https://www.djangoproject.com/" target="_blank"><img src="https://cdn.worldvectorlogo.com/logos/django.svg" alt="django" width="40" height="40"/></a>
</p>

<h3 align="left">üé® Design & Other Tools</h3>
<p align="left">
  <a href="https://figma.com/" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/figma/figma-original.svg" alt="figma" width="40" height="40"/></a>
  <a href="https://www.photoshop.com/" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/photoshop/photoshop-plain.svg" alt="photoshop" width="40" height="40"/></a>
  <a href="https://www.blender.org/" target="_blank"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/blender/blender-original.svg" alt="blender" width="40" height="40"/></a>
</p>

</p>

</p>


```sh
Oneday we will be released
```


**deep narrative workflow** 
Forming a unified, actionable pipeline for your dance recognition GCN module:

***

**Step 1: Set Up Project Environment for Model Development**  
Begin by creating a reproducible deep learning workspace (Colab, local GPU, or Dockerized Python setup). Install essential libraries: PyTorch (for GCN and neural network operations), PyTorch Geometric (for graph handling), NumPy/Pandas (for preprocessing), and Essentia (for extracting rhythm features). Clone the SMPL-X repository to enable expressive skeleton modeling relevant to breakdance movements and ensure compatibility with your BRACE dataset by validating with a sample keypoint sequence.

***

**Step 2: Preprocess BRACE Key-Points for Module Input**  
Load BRACE keypoint annotations (COCO-format JSONs) and preprocess them by normalizing coordinates for consistency across recordings. Calculate joint velocities to capture motion dynamics. Align available audio beat data to individual frames using Essentia, allowing for synchronized multimodal analysis. Store each sample as a structured tensor, ready for sequence batching and graph construction.

***

**Step 3: Define Initial GCN Module Architecture**  
Design your GCN model to accept per-frame keypoint features, optionally including audio beats and velocity. The input layer should match the output of your preprocessing (e.g., [num_frames, 69 features]). Architect the GCN with an initial graph layer that uses COCO skeleton edge definitions, followed by multiple spatial-temporal convolutional layers to model both pose and rhythm progression. Provide flexibility for future transformer-based extensions if temporal encoding becomes a bottleneck.

***

**Step 4: Tune GCN Hyperparameters and Add Regularization**  
Systematically experiment with key hyperparameters: set dropout between layers (typically 0.3‚Äì0.5) and apply L2 weight decay during optimization. Employ a dynamic learning rate scheduler such as ReduceLROnPlateau and implement early stopping if validation accuracy stalls. Incorporate augmentation by adding random jitter to keypoints (e.g., Gaussian noise, std=0.05), boosting robustness. Closely monitor both train and validation loss, accuracy, and variance to catch overfitting early.

***

**Step 5: Pretraining GCN on NTU RGB+D Dataset**  
Download NTU RGB+D skeleton data to leverage transfer learning. Map NTU‚Äôs 25-joint skeleton to BRACE/COCO‚Äôs 17-joint format by carefully aligning joint definitions and optionally averaging when direct correspondence is missing. Pretrain your GCN backbone on NTU, freeze the initial convolution layers, and fine-tune on your BRACE dataset. This hybrid training strategy enables the model to benefit from NTU‚Äôs generic motion coverage while adapting to specific dance styles in your dataset.

***

**Step 6: Integrate Essentia Beats and Test Multi-Model GCN**  
Extract beat onset strength and rhythm metadata for your sequences using Essentia. Augment input features for each frame with audio beat strengths. Test multi-modal variants of your GCN model by concatenating these features and evaluating their impact on validation accuracy. Visualize the temporal alignment of beats to keypoints to ensure correct multimodal input structure and iterate based on model performance.

***

**Addon: Advanced Dance Pipeline Steps (merged into workflow):**  
- After initial setup, **solidify BRACE data prep** by fully normalizing keypoints, calculating velocities, and concatenating features into the expected tensor shape.
- **Upgrade to improved GCN**: Extend to a three-layer architecture with dropout for stronger temporal and spatial abstraction.
- **Integrate augmentation directly** into dataloaders, ensuring every training batch is exposed to slight randomization.
- **Implement detailed metrics tracking**: Log all relevant metrics (training/validation loss, accuracy, loss variance) and visualize them for robust model diagnostics.
- **Build a NTU dataset class** to facilitate joint mapping and graph construction, including repeated edges for enhanced temporal graph modeling.

***

**This workflow is tailored for your project goals: it combines advanced regularization, transfer learning, data fusion (auditory and pose), and careful graph topology management to maximize dance motion classification performance and research validity.**

[1](https://www.notion.so/Prioritized-tasks-to-kickstart-module-creation-286143945312807684b8d09ff58a69fb)
